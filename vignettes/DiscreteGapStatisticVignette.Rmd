---
title: "DiscreteGapStatistic"
author: "Eduardo Cortes, Jeffrey Miecznikowski"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: mybib.bib
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Here we introduce:

- The main features of the `DiscreteGapStatistic` package via a simple data example.
- Two plots to summarize and visualize Likert or multivariate categorical data.
- Explain basic usage of the `clusGapDiscr` function.
- Perform simulation experiment that reproducing one of the scenarios found in [@cortes2024Discrete]. These exercise is intended to help extend this tool's usage.

# Installation

```{r Install, echo=TRUE, eval=FALSE, class.source = 'fold-show'}
install.packages('devtools')
devtools::install_github("ecortesgomez/DiscreteGapStatistic")
library('DiscreteGapStatistic')
```

```{r GhostLibs, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
library('DiscreteGapStatistic')
library('knitr')
library('kableExtra')
library('dplyr')
```

# Preliminaries

The MathAnxiety dataset can be found in the `likert` package [@speerschneider2013likert]. After loading the dataset, some light formatting is done to produce a `data.frame` called `massData`. The possible categories are 'Strongly Disagree', 'Disagree', 'Neutral' , 'Agree', 'Strongly Agree'. Notice that the categories are not transformed to numerical values, but are abbreviated for plotting purposes to DS, D, N, A, SA. Category ordering is preserved using the `factor` function. 

```{r LoadLibs, echo=TRUE, eval=FALSE}
## Load useful libraries

library('knitr')
library('kableExtra')
library('dplyr')
```


```{r LoadData, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
## Load preliminary libraries

library(likert)
data(mass)

massQ <- colnames(mass)
massQsh <- c('Gender',
             'Q1: Math Interesting', 'Q2: Uptight Math Test', 'Q3: Use Math In Future',
             'Q4: Mind Goes Blank', 'Q5: Math Relates to Life', 'Q6: Ability Solve Math Probls',
             'Q7: Sinking Feeling', 'Q8: Math Challenging', 'Q9: Math Makes Me Nervous',
             'Q10: Take More Math Classes', 'Q11: Math Makes Me Uneasy','Q12: Math Favorite Subj.',
             'Q13: Enjoy Learning Math', 'Q14: Math Makes Feel Confused')

massData <- mass
colnames(massData) <- massQsh
Cats <- setNames(object = c('SD', 'D', 'N', 'A', 'SA'),
                 nm = c('Strongly Disagree', 'Disagree', 
                        'Neutral', 
                        'Agree', 'Strongly Agree') )
massData <- data.frame(Gender = massData$Gender,
                       apply(massData[, -1], 2,
                             function(x) Cats[as.character(x)] %>%
                                factor(levels = Cats)),
                       check.names=FALSE)

rownames(massData) <- c(paste0('F', 1:2), paste0('M', 1),
                        paste0('F', 3:11), paste0('M', 2),
                        paste0('F', 12), paste0('M', 3),
                        paste0('F', 13), paste0('M', 4:5),
                        paste0('F', 14), paste0('M', 6))

## head(massData)
```

# Summarization and Data Exploration

Visual preliminary functions are introduced to explore the data before running the main function. 

## Likert Heatmap {#LikertHeat}

The first function used is an adaptation of the `likert.heatmap` found in the `likert` R package. `likert.heat.plot2` has the same arguments as the original function, but has an additional parameter, `allLevels`, where category order is specified as a character vector. Another difference in this function is the output. Since the original function numerically codifies categories, mean and variance statistics are produced at the left-hand side of the plot. These statistics will not be present in plots produced by `likert.heat.plot2`.

```{r LikertHeat,  out.width=600, dpi=500, fig.retina=NULL, echo=TRUE, fig.align = 'center'}
likert.heat.plot2(massData[, -1],
                  allLevels = Cats,
                  text.size = 1.5)+
   labs(title = 'Math Anxiety Data Likert Heatmap Summary')+
   theme(axis.text = element_text(size = 5), 
         title = element_text(size = 5))
```

## Distance matrices

In [@cortes2024Discrete] five categorical distance functions are introduced to quantify dissimilarities/discrepancies between two categorical vectors (see function `distancematrix`). The following table describes the available distances and the names used within the package.

```{r Distances, fig.align = 'center', echo=FALSE}
data.frame(Distance = c('Hamming', 'chi-square', "Cramer's V", "Hellinger", 'Bhattacharyya'), 
           Name = c('hamming', 'chisquare', 'cramerV', 'hellinger', 'bhattacharyya')) %>%
   kbl() %>%
   kable_paper(full_width = FALSE) %>%
   column_spec(column = 1, border_left = TRUE) %>%
   column_spec(column = 2, monospace = TRUE, border_left = TRUE, border_right=TRUE)
```

The resulting distance matrix from a rectangular dataset can easily be displayed and organized using heatmaps. The following plots visualize the `massData` using the defined distances with the function `distanceHeat`. This function only requires the dataset object and the name of the distance. It can also take advantage of the options and functionalities available in the `pheatmap` function (see the code below) from the `pheatmap` R package [@kolde2019]. For instance, by default, the columns are clustered using `clustering_method = 'complete'` but this parameter can be specified according to `pheatmap`'s options. Different aesthetic options are exemplified in the plots below using the introduced distances. 

```{r DistanceMats1, out.width=330, dpi=500, echo=TRUE, results='hold', fig.show="hold"}
distanceHeat(x = massData[, -1], distName = 'hamming',
             main = 'Hamming Distance\nMath Anxiety Data', fontsize = 6.5)

distanceHeat(x = massData[, -1], distName = 'cramerV',
             main = "Cramer's V Distance\nMath Anxiety Data", fontsize = 6.5, 
             show_rownames = FALSE, cluster_rows = FALSE, cluster_cols=FALSE)
```

```{r DistanceMats2, out.width=330, dpi=500, echo=TRUE, results='hold', fig.show="hold"}
distanceHeat(x = massData[, -1], distName = 'hellinger',
             main = 'Hellinger Distance\nMath Anxiety Data', fontsize = 6.5, 
             show_rownames = TRUE, border_color = 'black', 
             , cluster_rows = FALSE, cluster_cols=FALSE)

distanceHeat(x = massData[, -1], distName = 'bhattacharyya',
             main = 'Bhattacharyya Distance\nMath Anxiety Data', fontsize = 6.5, 
              show_rownames = TRUE, border_color = 'lightgrey', 
             , cluster_rows = FALSE, cluster_cols=FALSE)
```

<!--
```{r DistanceMats3, out.width=350, dpi=500, echo=TRUE, results='hold', fig.show="hold", fig.align = 'center'}
distanceHeat(x = massData[, -1], distName = 'chisquare',
             main = 'Chi-square Distance\nMath Anxiety Data', fontsize = 7, 
             cutree_cols = 2)
``` 
-->

# `clusGapDiscr`: The Gap Statistic for discrete data {#chiclust}

The Gap Statistic (GS) in its original formulation by [@tibshirani2001estimating] aims to determine an optimal number of clusters given a rectangular dataset with continuous columns and a pre-specified clustering method given $k$ number of clusters (i.e. $k$-means clustering or partitioning around medoids (pam)). This is done by comparing the within-cluster dispersion of the data to a randomly generated reference null distribution of the data obtaining estimations via bootstrapping. The idea is to quantify whether the clustering structure observed in the data is considerably different compared to what would be expected by random chance. `clusGap` is a widely used implementation of the GS from the `cluster` package [@maechler2023cluster] with the following basic arguments:

```{r clusGap, echo=TRUE, eval=FALSE}
clusGap(x, FUNcluster, K.max, B = 100, 
        verbose = interactive(), ...)
```

- `x`: argument can be `data.frame` or `matrix` object where rows represent observations and columns correspond to data features or variables. This matrix is expected to be contain exclusively numerical input. 
- `FUNcluster`: clustering function accepting on its first argument a rectangular data object (like `x`) and a second one specifying a desired `k` number of clusters.  
- `K.max`: maximum number of clusters to consider.  
- `B`: integer indicating the number of times to perform the Montecarlo repetitions. 

The discrete GS (dGS) proposed applies and adapts the principle behind the original formulation, but for categorical data using a distance-based approach. The GS assumes that the distance between observations can be correctly described with the Euclidean distance, since it assumes the data is continuous. In the case of categorical data, this distance assumption is not appropriate and different class of distances needs to be defined. `clusGapDiscr` is the main function that performs and implements the dGS. The first four parameters found in `clusGap` are identical and have the same usage to the ones found in `clusGapDiscr`. Additionally, the function requires a categorical distance from the list mentioned above. Another feature of the dGS is the reference null distribution used, which can be a discrete uniform distribution of the unique column-wise categories found in the data; this setting is defined as the Data Support (DS). In other settings, the categories to be used must be user-specified via a character vector or list. This setting will be referred as the Known Support (KS). 

```{r clusGapDiscr, echo=TRUE, eval=FALSE}
clusGapDiscr(x, FUNcluster, K.max, B = nrow(x),
             distName, value.range = 'DS', 
             verbose = interactive(), ...)
```

- `distaName`: name of discrete distance. The available options are `'hamming'`, `'chisquare'`, `'cramersV'`, `'bhattacharyya'` and `'hellinger'`.

- `value.range`: specifies the values of the reference null distribution. Possible values: `'DS'` or a character vector with unique values. `'DS'` option extracts the unique values found in `x`. 

Similar to `clusGap`, `clusGapDiscr` returns a matrix object with `K.max` rows corresponding to the user-specified number of clusters with an estimate of the GS and its corresponding standard error. This information would then determine the number of clusters according to the cluster-selection criterion. The one used in `DiscreteGapStatistic` is the 1-SE criterion implemented by the function `findK`, which accepts `clusGap` objects. The following example applies the dGS on the Math Anxiety data using the Hamming, $\chi^2$, and Cramer's V distances applying the `cluster::pam` algorithm. A plot is then generated displaying the GS against the number of clusters. Each estimate is accompanied by a respective standard-error bars and a blue dashed line indicating the chosen number of clusters.  

```{r clusGapDiscrFit, echo=TRUE, eval=FALSE}
## Recall Cats:
# Cats <- setNames(object = c('SD', 'D', 'N', 'A', 'SA'),
#                  nm = c('Strongly Disagree', 'Disagree', 
#                         'Neutral', 
#                         'Agree', 'Strongly Agree') )

HammRun <- clusGapDiscr(x = massData[, -1],
                        FUNcluster = cluster::pam,
                        B = 100,
                        K.max = 9,
                        value.range = 'DS',
                        distName = 'hamming')

chisqRun <- clusGapDiscr(x = massData[, -1],
                        FUNcluster = cluster::pam,
                        B = 100,
                        K.max = 9,
                        value.range = Cats,
                        distName = 'chisquare')

crVRun <- clusGapDiscr(x = massData[, -1],
                        FUNcluster = cluster::pam,
                        B = 100,
                        K.max = 9,
                        value.range = 'DS',
                        distName = 'cramerV')

plot(HammRun,
     main = "Discrete Gap statistic: Hamming Distance\nMath Anxiety Data",
     cex = 2, cex.lab=1.2, cex.axis=1.5, cex.main=1.5)
abline(v = findK(HammRun), lty=3, lwd=2, col="Blue")

plot(chisqRun,
     main = "Discrete Gap statistic: chi-square Distance\nMath Anxiety Data",
     cex = 2, cex.lab=1.2, cex.axis=1.5, cex.main=1.5)
abline(v = findK(chisqRun), lty=3, lwd=2, col="Blue")

plot(crVRun,
     main = "Discrete Gap statistic: Cramer's V Distance\nMath Anxiety Data",
     cex = 2, cex.lab=1.2, cex.axis=1.5, cex.main=1.5)
abline(v = findK(crVRun), lty=3, lwd=2, col="Blue")
```

```{r clusGapDiscrFitPlots, echo=FALSE, eval=TRUE, out.width=210, dpi=500}
# png(filename = './vignettes/mass_HammRun.png',
#      width = 7, height = 7, res=500, units = 'in')
# plot(HammRun,
#      main = "Discrete Gap statistic: Hamming Distance\nMath Anxiety Data",
#      cex = 2, cex.lab=1.2, cex.axis=1.5, cex.main=1.5)
# abline(v = findK(HammRun), lty=3, lwd=2, col="Blue")
# dev.off()
# 
# png(filename = './vignettes/mass_chisqRun.png',
#      width = 7, height = 7, res=500, units = 'in')
# plot(chisqRun,
#      main = "Discrete Gap statistic: chi-square Distance\nMath Anxiety Data",
#      cex = 2, cex.lab=1.2, cex.axis=1.5, cex.main=1.5)
# abline(v = findK(chisqRun), lty=3, lwd=2, col="Blue")
# dev.off()
# 
# png(filename = './vignettes/mass_crVRun.png',
#      width = 7, height = 7, res=500, units = 'in')
# plot(crVRun,
#      main = "Discrete Gap statistic: Cramer's V Distance\nMath Anxiety Data",
#      cex = 2, cex.lab=1.2, cex.axis=1.5, cex.main=1.5)
# abline(v = findK(crVRun), lty=3, lwd=2, col="Blue")
# dev.off()
# save(HammRun, chisqRun, crVRun,
#      file = './vignettes/mass_SavedRuns.RData')
load(file = './mass_SavedRuns.RData')
include_graphics(paste0('./mass_', c('Hamm', 'chisq', 'crV'), 'Run.png'), dpi = 500)
```

Notice that since all possible categorical values are available in the data, using the option `value.range = Cats` would yield exact same results. 
<!-- The running time of `clusGapDiscr` depends on the chosen discrete distance. Overall, Hamming runs the fastest. Bhattacharyya, Hellinger and $\chi^2$ run at about the same speed, but slower than Hamming. The slowest is Cramer's V distance. -->

Heatmaps can be created to visualize the commonalities within each cluster and the differences between them.

```{r resHeatMaps, echo=TRUE, eval=FALSE, out.width=210, dpi=500}

ResHeatmap(massData[, -1],
           distName = 'hamming',
           catVals = Cats,
           nCl = findK(HammRun),
           out = 'heatmap',
           prefObs = NULL,
           filename = 'HM_MathAnxiety_Hamm',
           outDir = './')

ResHeatmap(massData[, -1],
           distName = 'chisquare',
           catVals = Cats,
           nCl = findK(chisqRun),
           out = 'heatmap',
           prefObs = NULL,
           filename = 'HM_MathAnxiety_chisq',
           outDir = './')

ResHeatmap(massData[, -1],
           distName = 'cramerV',
           catVals = Cats,
           nCl = findK(crVRun),
           out = 'heatmap',
           prefObs = NULL,
           filename = 'HM_MathAnxiety_crV',
           outDir = './')

include_graphics(paste0('./HM_MathAnxiety_', c('Hamm', 'chisq', 'crV'), '.png'), dpi = 500)
```

```{r resHeatMapsRun, echo=FALSE, eval=TRUE, out.width=215, dpi=500, message=FALSE, warning=FALSE}

# ResHeatmap(massData[, -1],
#            distName = 'hamming',
#            catVals = Cats,
#            nCl = findK(HammRun),
#            out = 'heatmap',
#            filename = 'HM_MathAnxiety_Hamm',
#            outDir = './')
# 
# ResHeatmap(massData[, -1],
#            distName = 'chisquare',
#            catVals = Cats,
#            nCl = findK(chisqRun),
#            out = 'heatmap',
#            filename = 'HM_MathAnxiety_chisq',
#            outDir = './')
# 
# ResHeatmap(massData[, -1],
#            distName = 'cramerV',
#            catVals = Cats,
#            nCl = findK(crVRun),
#            out = 'heatmap',
#            filename = 'HM_MathAnxiety_crV',
#            outDir = './')

include_graphics(paste0('./HM_MathAnxiety_', c('Hamm', 'chisq', 'crV'), '.png'), 
                 dpi = 500)
```

Notice that Hamming distance detects subclusters present in Cluster 2 from the $\chi^2$ distance run. To compare the similar clusters side-to-side, the parameter `nCl` can be used to reorder the clusters relative to the given ordering. This case `nCl = 2:1` will alter (invert) the order of the two clusters on the $\chi^2$ based cluster. A third heatmap is displayed relabeling the clusters from the $\chi^2$ run using the `clusterNames = 'renumber'` argument.

```{r resHeatMaps2, echo=TRUE, eval=FALSE, out.width=215, dpi=500, message=FALSE, warning=FALSE, error=FALSE}

ResHeatmap(massData[, -1],
           distName = 'hamming',
           catVals = Cats,
           nCl = findK(HammRun),
           out = 'heatmap',
           filename = 'HM_MathAnxiety_Hamm',
           outDir = './')

ResHeatmap(massData[, -1],
           distName = 'chisquare',
           catVals = Cats,
           nCl = 2:1,
           out = 'heatmap',
           filename = 'HM_MathAnxiety_chisq_Mod1',
           outDir = './')

ResHeatmap(massData[, -1],
           distName = 'chisquare',
           catVals = Cats,
           nCl = 2:1,
           clusterNames = 'renumber',
           out = 'heatmap',
           filename = 'HM_MathAnxiety_chisq_Mod2',
           outDir = './')

include_graphics(paste0('./HM_MathAnxiety_', c('Hamm', 'chisq_Mod1', 'chisq_Mod2'), '.png'), 
                 dpi = 500)
```


```{r resHeatMaps2Run, echo=FALSE, eval=TRUE, out.width=215, dpi=500, message=FALSE, warning=FALSE, error=FALSE}

# ResHeatmap(massData[, -1],
#            distName = 'hamming',
#            catVals = Cats,
#            nCl = findK(HammRun),
#            out = 'heatmap',
#            filename = 'HM_MathAnxiety_Hamm',
#            outDir = './')
# 
# ResHeatmap(massData[, -1],
#            distName = 'chisquare',
#            catVals = Cats,
#            nCl = 2:1,
#            out = 'heatmap',
#            filename = 'HM_MathAnxiety_chisq_Mod1',
#            outDir = './')
# 
# ResHeatmap(massData[, -1],
#            distName = 'chisquare',
#            catVals = Cats,
#            nCl = 2:1,
#            clusterNames = 'renumber',
#            out = 'heatmap',
#            filename = 'HM_MathAnxiety_chisq_Mod2',
#            outDir = './')

include_graphics(paste0('./HM_MathAnxiety_', c('Hamm', 'chisq_Mod1', 'chisq_Mod2'), '.png'), 
                 dpi = 500)
```

# Evaluating a simple simulation scenario

This example reproduces results from the simulation study found in [@cortes2024Discrete] .
For this simplified setting, the group responses are determined by different multinomial probability vectors across questions. The simulation scenario is described as two different clusters generated by $\boldsymbol{\pi}_1$ and $\boldsymbol{\pi}_2$, $n_r = 50$, $q = 10$ for the $\chi^2$ and Bhattacharyya distances using the $DS$ null mode. Here both groups have highest densities concentrated in $c0$, but group1's support is limited to $c0$, $c1$ and $c2$, whereas, group2 spreads from categories $c0$ to $c6$. $100$ Monte Carlo repetitions are performed here. The following code configures the scenarios.

```{r SettingUpSim1, echo=TRUE, eval=TRUE, dpi=500, message=FALSE, warning=FALSE}
## Define pi1 and pi2 vectors
pi1 <- setNames(c(0.8, 0.1, 0.1, 0, 0, 0, 0), paste0('c', 0:6))
pi2 <- setNames(c(0.4, 0.2, 0.2, 0.05, 0.05, 0.05, 0.05), paste0('c', 0:6))

## Number of Monte-Carlo repetitions
nMC <- 100

## Build data.frame with both scenarios.
scenL <- list(Bhatt = list(SimN=1, nGr= 2, nQ=10, nr = 30, dist = 'bhattacharyya',
                    nullDistr = 'DS', maxK = 9 ), 
              Chisq = list(SimN=2, nGr= 2, nQ=10, nr = 30, dist = 'chisquare', 
                    nullDistr = 'DS', maxK = 9 ))
```

The following code first performs the Monte Carlo simulations, retrieves results and plots them using stacked bars. The first piece of code generates data using the internal `SimData` function requiring `N` number of observations, `nQ` number of questions and a probability vector. Then `clusGapDiscr` function is called using the parameters defined above. The result would a be a `list` with the chosen number of clusters for each repetition. A `data.frame` formats the results to be displayed via `ggplot2`.

```{r Sim1, echo=TRUE, eval=FALSE}
library(parallel)
## detectCores()
## nThr <- 10 ## Select number of threads available

SimBhatt <- mclapply(1:nMC,
                  function(ii){
                           SimDat <- rbind(with(scenL$Bhatt,
                                                SimData(N = nr, nQ = nQ, pi = pi1)), 
                                           with(scenL$Bhatt,
                                                SimData(N = nr, nQ = nQ, pi = pi2)))
                           
                           cGres <- clusGapDiscr(x = SimDat,
                                                  FUNcluster = cluster::pam,
                                                  K.max = scenL$Bhatt['maxK'] %>% as.integer,
                                                  value.range = scenL$Bhatt['nullDistr'] %>% 
                                                    as.character,
                                                  distName = scenL$Bhatt['dist'])

                           try(findK(cG_obj = cGres))
                       },
                       mc.cores = nThr)

SimChisq <- mclapply(1:nMC,
                  function(ii){
                           SimDat <- rbind(with(scenL$Chisq,
                                                SimData(N = nr, nQ = nQ, pi = pi1)), 
                                           with(scenL$Chisq,
                                                SimData(N = nr, nQ = nQ, pi = pi2)))
                           
                           cGres <- clusGapDiscr(x = SimDat,
                                                  FUNcluster = cluster::pam,
                                                  K.max = scenL$Chisq['maxK'] %>% as.integer,
                                                  value.range = scenL$Chisq['nullDistr'] %>% 
                                                    as.character,
                                                  distName = scenL$Chisq['dist'])

                           try(findK(cG_obj = cGres))
                       },
                       mc.cores = nThr)

resSim <- rbind(SimBhatt %>% unlist %>% table %>% data.frame %>%
                     mutate(dist = 'Bhattacharyya'), 
               SimChisq %>% unlist %>% table %>% data.frame %>%
                     mutate(dist = 'Chisquare')) %>%
      dplyr::rename(c(nClust = '.')) %>%
      mutate(nClust = as.integer(nClust)) %>%
      mutate(nClust = ifelse(nClust > 4 , '>4', nClust) %>%
                     factor(levels = c(1:4, '>4') %>% rev), 
             Freq = Freq/100)
   
fillCols <- setNames(c('grey80', 'firebrick', 'grey70', 'grey50', 'grey40'), c(1:4, '>4'))

ggplot(resSim, aes(x=dist, y=Freq, fill = nClust)) +
   geom_bar(stat = 'identity', width = 0.7)+
   scale_fill_manual(values = fillCols) +
   theme_bw() +
   labs(title = 'Simulation Results', 
        x = 'Categorical Distance', y = 'Probability', 
        fill = 'nClust')
```

```{r Sim1Run, echo=FALSE, eval=TRUE, fig.align='center', out.width=400, dpi=500}
## save(SimBhatt, SimChisq,
load('./QuickSimRun.RData')

resSim <- rbind(SimBhatt %>% unlist %>% table %>% data.frame %>%
                     mutate(dist = 'Bhattacharyya'), 
               SimChisq %>% unlist %>% table %>% data.frame %>%
                     mutate(dist = 'Chisquare')) %>%
      dplyr::rename(c(nClust = '.')) %>%
      mutate(nClust = as.integer(nClust)) %>%
      mutate(nClust = ifelse(nClust > 4 , '>4', nClust) %>%
                     factor(levels = c(1:4, '>4') %>% rev), 
             Freq = Freq/100)
   
fillCols <- setNames(c('grey80', 'firebrick', 'grey70', 'grey50', 'grey40'), c(1:4, '>4'))

ggplot(resSim, aes(x=dist, y=Freq, fill = nClust)) +
   geom_bar(stat = 'identity', width = 0.7)+
   scale_fill_manual(values = fillCols) +
   theme_bw() +
   labs(title = 'Simulation Results', 
        x = 'Categorical Distance', y = 'Probability', 
        fill = 'nClust') 
```

The stacked-bar plot summarizes the results of the simulations. The red color highlights correct number of cluster choices, while different tones of grey indicate the wrong choices. Here the two distances perform similarly, except the case where $\chi^2$ distance does not underestimate number of clusters compared to the Bhattacharyya distance, choosing one cluster ($k=1$) $7\%$ of the time.


# References
